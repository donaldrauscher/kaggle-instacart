{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BQ table with data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client()\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "table_ref = bq_client.dataset('instacart').table('reorder_model')\n",
    "job_config.destination = table_ref\n",
    "job_config.write_disposition = 'WRITE_TRUNCATE'\n",
    "\n",
    "query = \"\"\"\n",
    "    WITH users AS (\n",
    "      SELECT user_id, COUNT(*) AS num_orders, SUM(days_since_prior_order) AS days_bw_first_last_order\n",
    "      FROM instacart.orders\n",
    "      WHERE eval_set = \"prior\"\n",
    "      GROUP BY 1\n",
    "    ), user_product AS (\n",
    "      SELECT orders.user_id, op.product_id, \n",
    "        COUNT(*) AS num_orders, SUM(op.reordered) AS num_reorders,\n",
    "        MIN(orders.order_number) AS first_order_number, MIN(days_since_first_order) AS first_order_day,\n",
    "        MAX(orders.order_number) AS last_order_number, MAX(days_since_first_order) AS last_order_day,\n",
    "        AVG(op.add_to_cart_order) AS avg_cart_order\n",
    "      FROM instacart.order_products__prior AS op\n",
    "      INNER JOIN (\n",
    "        SELECT *,\n",
    "          SUM(COALESCE(days_since_prior_order,0)) OVER (PARTITION BY user_id ORDER BY order_number ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS `days_since_first_order`\n",
    "        FROM instacart.orders \n",
    "        WHERE eval_set = \"prior\"\n",
    "      ) AS orders USING(order_id)\n",
    "      GROUP BY 1,2\n",
    "    ), user_product_features AS (\n",
    "      SELECT up.user_id, up.product_id,\n",
    "        up.num_orders / users.num_orders AS perc_all_orders,\n",
    "        SAFE_DIVIDE(up.num_reorders, users.num_orders - up.first_order_number) AS perc_reorder,\n",
    "        SAFE_DIVIDE(up.num_orders, users.days_bw_first_last_order) AS orders_per_day,\n",
    "        SAFE_DIVIDE(up.num_reorders, users.days_bw_first_last_order - up.first_order_day) AS reorders_per_day,\n",
    "        up.first_order_number, up.first_order_day, up.last_order_number, up.last_order_day, up.avg_cart_order, \n",
    "        users.days_bw_first_last_order\n",
    "      FROM user_product AS up\n",
    "      INNER JOIN users AS users USING(user_id)\n",
    "    ), user_features AS (\n",
    "      SELECT orders.user_id,\n",
    "        ANY_VALUE(users.num_orders) AS num_orders,\n",
    "        ANY_VALUE(users.days_bw_first_last_order) AS days_bw_first_last_order,\n",
    "        ANY_VALUE(users.days_bw_first_last_order) / ANY_VALUE(users.num_orders) AS avg_days_bw_orders,\n",
    "        COUNT(*) / ANY_VALUE(users.num_orders) AS num_products_per_order,\n",
    "        SUM(op.reordered) / SUM(CASE WHEN orders.order_number > 1 THEN 1 ELSE 0 END) AS perc_reorder,\n",
    "        COUNT(DISTINCT op.product_id) AS num_products,\n",
    "        COUNT(DISTINCT products.aisle_id) AS num_aisles,\n",
    "        COUNT(DISTINCT products.department_id) AS num_departments\n",
    "      FROM instacart.orders AS orders\n",
    "      INNER JOIN instacart.order_products__prior AS op USING(order_id)\n",
    "      INNER JOIN instacart.products AS products USING(product_id)\n",
    "      INNER JOIN users USING(user_id)\n",
    "      GROUP BY 1\n",
    "    ), product_features AS (\n",
    "      SELECT product_id, aisle_id, department_id,\n",
    "        num_users / num_users_tot AS perc_users,\n",
    "        num_orders / num_orders_tot AS perc_all_orders,\n",
    "        num_reorder / num_reorder_tot AS perc_reorder\n",
    "      FROM (\n",
    "        SELECT products.product_id, products.aisle_id, products.department_id,\n",
    "          COUNT(DISTINCT orders.user_id) AS num_users,\n",
    "          COUNT(*) AS num_orders, \n",
    "          SUM(op.reordered) AS num_reorder\n",
    "        FROM instacart.orders AS orders\n",
    "        INNER JOIN instacart.order_products__prior AS op USING(order_id)\n",
    "        INNER JOIN instacart.products AS products USING(product_id)\n",
    "        GROUP BY 1,2,3\n",
    "      ) AS x\n",
    "      INNER JOIN (\n",
    "        SELECT COUNT(DISTINCT user_id) AS num_users_tot,\n",
    "          COUNT(*) AS num_orders_tot, \n",
    "          SUM(CASE WHEN order_number > 1 THEN 1 ELSE 0 END) AS num_reorder_tot\n",
    "        FROM instacart.orders\n",
    "        WHERE eval_set = \"prior\"\n",
    "      ) AS y ON 1=1\n",
    "    ), all_features AS (\n",
    "      SELECT\n",
    "        upf.user_id,\n",
    "        upf.product_id,\n",
    "        pf.aisle_id,\n",
    "        pf.department_id,\n",
    "        upf.perc_all_orders AS upf_perc_all_orders,\n",
    "        upf.perc_reorder AS upf_perc_reorder,\n",
    "        upf.orders_per_day AS upf_orders_per_day,\n",
    "        upf.reorders_per_day AS upf_reorders_per_day,\n",
    "        upf.first_order_number AS upf_first_order_number,\n",
    "        upf.first_order_day AS upf_first_order_day,\n",
    "        upf.last_order_number AS upf_last_order_number,\n",
    "        upf.last_order_day AS upf_last_order_day,\n",
    "        upf.avg_cart_order AS upf_avg_cart_order,\n",
    "        uf.num_orders AS uf_num_orders,\n",
    "        uf.num_products_per_order AS uf_num_products_per_order,\n",
    "        uf.perc_reorder AS uf_perc_reorder,\n",
    "        uf.days_bw_first_last_order AS uf_days_bw_first_last_order,\n",
    "        uf.avg_days_bw_orders AS uf_avg_days_bw_orders,\n",
    "        uf.num_products AS uf_num_products,\n",
    "        uf.num_aisles AS uf_num_aisles,\n",
    "        uf.num_departments AS uf_num_departments,\n",
    "        pf.perc_users AS pf_perc_users,\n",
    "        pf.perc_all_orders AS pf_perc_all_orders,\n",
    "        pf.perc_reorder AS pf_perc_reorder\n",
    "      FROM user_product_features AS upf\n",
    "      INNER JOIN user_features AS uf USING(user_id)\n",
    "      INNER JOIN product_features AS pf USING(product_id)\n",
    "    )\n",
    "    SELECT af.*, \n",
    "      # a few other features that need to computed based on order\n",
    "      af.uf_days_bw_first_last_order - af.upf_last_order_day + o.days_since_prior_order AS upf_days_since_last_order,\n",
    "      o.order_number - af.upf_last_order_number AS upf_orders_since_last_order,\n",
    "      # train vs. test and reordered (only for train)\n",
    "      o.eval_set,\n",
    "      o.order_id,\n",
    "      CASE WHEN o.eval_set = \"test\" THEN NULL ELSE LEAST(COALESCE(op_train.order_id,0),1) END AS reordered\n",
    "    FROM all_features AS af\n",
    "    INNER JOIN instacart.orders AS o ON af.user_id = o.user_id AND o.eval_set IN ('train','test')\n",
    "    LEFT JOIN instacart.order_products__train AS op_train ON o.order_id = op_train.order_id AND af.product_id = op_train.product_id\n",
    "\"\"\"\n",
    "\n",
    "query_job = bq_client.query(query, job_config=job_config)\n",
    "result = query_job.result(timeout=600)\n",
    "assert query_job.state == 'DONE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull data from BQ into Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for deleting temp files when we're done\n",
    "def cleanup(sess, input_directory):\n",
    "    input_path = sess._jvm.org.apache.hadoop.fs.Path(input_directory)\n",
    "    input_path.getFileSystem(sess._jsc.hadoopConfiguration()).delete(input_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up spark session\n",
    "sess = SparkSession.builder\\\n",
    "    .appName(\"Model builder\")\\\n",
    "    .config(\"spark.executor.cores\", 2)\\\n",
    "    .config(\"spark.executor.memory\", \"7g\")\\\n",
    "    .config(\"spark.network.timeout\", 3000)\\\n",
    "    .config(\"spark.shuffle.io.maxRetries\", 10)\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "bucket = sess._sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')\n",
    "project = sess._sc._jsc.hadoopConfiguration().get('fs.gs.project.id')\n",
    "input_dir = 'gs://{}/hadoop/tmp/bigquery/pyspark_input'.format(bucket)\n",
    "output = 'gs://instacart-data/outputs/reorder_test_pred.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from bq\n",
    "conf = {\n",
    "    'mapred.bq.project.id': project,\n",
    "    'mapred.bq.gcs.bucket': bucket,\n",
    "    'mapred.bq.temp.gcs.path': input_dir,\n",
    "    'mapred.bq.input.project.id': project,\n",
    "    'mapred.bq.input.dataset.id': 'instacart',\n",
    "    'mapred.bq.input.table.id': 'reorder_model',\n",
    "}\n",
    "\n",
    "cleanup(sess, input_dir)\n",
    "\n",
    "data_raw = sess._sc.newAPIHadoopRDD(\n",
    "    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'com.google.gson.JsonObject',\n",
    "    conf=conf)\n",
    "\n",
    "data_json = data_raw.map(lambda x: x[1])\n",
    "data_df = sess.read.json(data_json).repartition(sess._sc.defaultParallelism*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast integers\n",
    "data_df = data_df\\\n",
    "    .withColumn('label', col(\"reordered\").cast('integer'))\\\n",
    "    .withColumn('aisle_id', col(\"aisle_id\").cast('integer'))\\\n",
    "    .withColumn('department_id', col(\"department_id\").cast('integer'))\\\n",
    "    .withColumn('user_id', col(\"user_id\").cast('integer'))\\\n",
    "    .withColumn('product_id', col(\"product_id\").cast('integer'))\\\n",
    "    .withColumn('order_id', col(\"order_id\").cast('integer'))\\\n",
    "    .withColumn('uf_num_orders', col(\"uf_num_orders\").cast('integer'))\\\n",
    "    .withColumn('uf_days_bw_first_last_order', col(\"uf_days_bw_first_last_order\").cast('integer'))\\\n",
    "    .withColumn('uf_num_aisles', col(\"uf_num_aisles\").cast('integer'))\\\n",
    "    .withColumn('uf_num_departments', col(\"uf_num_departments\").cast('integer'))\\\n",
    "    .withColumn('uf_num_products', col(\"uf_num_products\").cast('integer'))\\\n",
    "    .withColumn('upf_first_order_day', col(\"upf_first_order_day\").cast('integer'))\\\n",
    "    .withColumn('upf_first_order_number', col(\"upf_first_order_number\").cast('integer'))\\\n",
    "    .withColumn('upf_last_order_day', col(\"upf_last_order_day\").cast('integer'))\\\n",
    "    .withColumn('upf_last_order_number', col(\"upf_last_order_number\").cast('integer'))\\\n",
    "    .withColumn('upf_orders_since_last_order', col(\"upf_orders_since_last_order\").cast('integer'))\\\n",
    "    .withColumn('upf_days_since_last_order', col(\"upf_days_since_last_order\").cast('integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split and set up ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "train = data_df.filter(data_df.eval_set == 'train').cache()\n",
    "test = data_df.filter(data_df.eval_set == 'test').cache()\n",
    "\n",
    "train_user, validate_user = train.select('user_id').distinct().randomSplit([0.8, 0.2], seed=1)\n",
    "\n",
    "train2 = train.join(broadcast(train_user), 'user_id').cache()\n",
    "validate = train.join(broadcast(validate_user), 'user_id').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct pipeline\n",
    "xvar1 = [\"upf_perc_all_orders\", \"upf_perc_reorder\", \"upf_orders_per_day\", \"upf_reorders_per_day\", \\\n",
    "         \"upf_first_order_number\", \"upf_first_order_day\", \"upf_last_order_number\", \"upf_last_order_day\", \\\n",
    "         \"upf_avg_cart_order\", \"upf_days_since_last_order\", \"upf_orders_since_last_order\"]\n",
    "\n",
    "xvar2 = [\"uf_num_orders\", \"uf_num_products_per_order\", \"uf_perc_reorder\", \\\n",
    "         \"uf_days_bw_first_last_order\", \"uf_avg_days_bw_orders\", \"uf_num_products\", \"uf_num_aisles\", \\\n",
    "         \"uf_num_departments\"]\n",
    "\n",
    "xvar3 = [\"pf_perc_users\", \"pf_perc_all_orders\", \"pf_perc_reorder\"]\n",
    "\n",
    "xvar4 = [\"aisle_id\", \"department_id\"]\n",
    "\n",
    "xvar = xvar1 + xvar2 + xvar3 + xvar4\n",
    "\n",
    "null_counts = train.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in train.columns))\\\n",
    "                .toPandas().transpose()\n",
    "null_col = list(null_counts.index[null_counts[0].nonzero()])\n",
    "\n",
    "imp = Imputer(strategy=\"median\", inputCols=null_col, outputCols=null_col)\n",
    "va = VectorAssembler(inputCols=xvar, outputCol=\"features\")\n",
    "gb = GBTClassifier(seed=0, maxIter=10)\n",
    "pipeline = Pipeline(stages=[imp, va, gb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.8279589610834616\n",
      "Best CV param: {Param(parent='GBTClassifier_40d0a7396b9c4171e238', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 25, Param(parent='GBTClassifier_40d0a7396b9c4171e238', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 7, Param(parent='GBTClassifier_40d0a7396b9c4171e238', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.2}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "param_grid = ParamGridBuilder()\\\n",
    "    .addGrid(gb.minInstancesPerNode, [10, 25])\\\n",
    "    .addGrid(gb.maxDepth, [5, 7])\\\n",
    "    .addGrid(gb.stepSize, [0.1, 0.2])\\\n",
    "    .build()\n",
    "\n",
    "eva = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    numFolds=3,\n",
    "                    evaluator=eva)\n",
    "\n",
    "cv_model = cv.fit(train2)\n",
    "\n",
    "best_func = np.argmax if eva.isLargerBetter() else np.argmin\n",
    "best_idx = best_func(cv_model.avgMetrics)\n",
    "best_score = cv_model.avgMetrics[best_idx]\n",
    "best_param = param_grid[best_idx]\n",
    "\n",
    "print(\"Best CV score: {}\".format(best_score))\n",
    "print(\"Best CV param: {}\".format(best_param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine cutoff which maximizes mean F1 score\n",
    "\n",
    "This model focuses on predicting reorders of products ordered previously.  We need to include products ordered for the first time, for which we are not generating predictions, in the recall denominator.  Otherwise, we will get a F1 estimate that is too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of products that are reorders\n",
    "query = \"SELECT AVG(reordered) FROM instacart.order_products__train\"\n",
    "query_job = bq_client.query(query)\n",
    "prop_reordered = list(query_job.result())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate probabilities for validation set\n",
    "true_prob = udf(lambda x: float(x[-1]))\n",
    "\n",
    "validate_pred = cv_model.transform(validate, best_param)\n",
    "validate_pred = validate_pred.select(true_prob('probability').alias('probability').cast('float'), 'label')\n",
    "\n",
    "validate_pred = validate_pred.withColumn(\"probability_bkt\", round(col(\"probability\"), 2))\n",
    "validate_pred_df = validate_pred.groupBy(\"probability_bkt\")\\\n",
    "                                .agg(sum('label').alias('sum'), count('label').alias('count'))\\\n",
    "                                .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.16\n",
      "Optimal threshold F1: 0.3407145351781795\n"
     ]
    }
   ],
   "source": [
    "# calculate precision/recall at different thresholds\n",
    "def precision_fn(df, cutoff): \n",
    "    x = df.loc[df.probability_bkt >= cutoff, ['sum','count']].apply(np.sum)\n",
    "    return x[0] / x[1]\n",
    "\n",
    "def recall_fn(df, cutoff):\n",
    "    relevant = np.sum(df['sum']) / prop_reordered\n",
    "    return np.sum(df['sum'][df.probability_bkt >= cutoff]) / relevant\n",
    "\n",
    "thresholds = np.arange(0.01, 0.99, 0.01)\n",
    "precision = np.array([precision_fn(validate_pred_df, x) for x in thresholds])\n",
    "recall = np.array([recall_fn(validate_pred_df, x) for x in thresholds])\n",
    "f1 = (2*precision*recall)/(precision+recall)\n",
    "optimal_threshold = thresholds[np.nanargmax(f1)]\n",
    "\n",
    "print(\"Optimal threshold: {}\".format(optimal_threshold))\n",
    "print(\"Optimal threshold F1: {}\".format(np.nanmax(f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune model on entire data\n",
    "model = pipeline.fit(train, best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions for test set\n",
    "collapse = udf(lambda x: ' '.join([str(i) for i in x]))\n",
    "\n",
    "test_order = test.select(\"order_id\").distinct()\n",
    "\n",
    "test_pred = model.transform(test)\n",
    "true_prob = udf(lambda x: float(x[-1]))\n",
    "\n",
    "test_pred = test_pred.select('order_id', 'product_id', true_prob('probability').alias('probability').cast('float'))\\\n",
    "                     .filter(col(\"probability\") >= optimal_threshold)\\\n",
    "                     .groupBy('order_id').agg(collect_list('product_id').alias('products'))\n",
    "test_pred = test_pred.withColumn('products', collapse('products'))\n",
    "test_pred = test_order.join(test_pred, on='order_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "cleanup(sess, output)\n",
    "test_pred.repartition(1).write.option('header', 'true').csv(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "cleanup(sess, input_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
