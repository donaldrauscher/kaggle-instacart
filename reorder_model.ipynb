{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BQ table with data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7f3e5dd84400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_client = bigquery.Client()\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "table_ref = bq_client.dataset('instacart').table('reorder_model')\n",
    "job_config.destination = table_ref\n",
    "job_config.write_disposition = 'WRITE_TRUNCATE'\n",
    "\n",
    "query = \"\"\"\n",
    "    WITH users AS (\n",
    "      SELECT user_id, COUNT(*) AS num_orders, SUM(days_since_prior_order) AS days_bw_first_last_order\n",
    "      FROM instacart.orders\n",
    "      WHERE eval_set = \"prior\"\n",
    "      GROUP BY 1\n",
    "    ), user_product AS (\n",
    "      SELECT orders.user_id, op.product_id, \n",
    "        COUNT(*) AS num_orders, SUM(op.reordered) AS num_reorders,\n",
    "        MIN(orders.order_number) AS first_order_number, MIN(days_since_first_order) AS first_order_day,\n",
    "        MAX(orders.order_number) AS last_order_number, MAX(days_since_first_order) AS last_order_day,\n",
    "        AVG(op.add_to_cart_order) AS avg_cart_order\n",
    "      FROM instacart.order_products__prior AS op\n",
    "      INNER JOIN (\n",
    "        SELECT *,\n",
    "          SUM(COALESCE(days_since_prior_order,0)) OVER (PARTITION BY user_id ORDER BY order_number ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS `days_since_first_order`\n",
    "        FROM instacart.orders \n",
    "        WHERE eval_set = \"prior\"\n",
    "      ) AS orders USING(order_id)\n",
    "      GROUP BY 1,2\n",
    "    ), user_product_features AS (\n",
    "      SELECT up.user_id, up.product_id,\n",
    "        up.num_orders / users.num_orders AS perc_all_orders,\n",
    "        SAFE_DIVIDE(up.num_reorders, users.num_orders - up.first_order_number) AS perc_reorder,\n",
    "        SAFE_DIVIDE(up.num_orders, users.days_bw_first_last_order) AS orders_per_day,\n",
    "        SAFE_DIVIDE(up.num_reorders, users.days_bw_first_last_order - up.first_order_day) AS reorders_per_day,\n",
    "        up.first_order_number, up.first_order_day, up.last_order_number, up.last_order_day, up.avg_cart_order, \n",
    "        users.days_bw_first_last_order\n",
    "      FROM user_product AS up\n",
    "      INNER JOIN users AS users USING(user_id)\n",
    "    ), user_features AS (\n",
    "      SELECT orders.user_id,\n",
    "        ANY_VALUE(users.num_orders) AS num_orders,\n",
    "        ANY_VALUE(users.days_bw_first_last_order) AS days_bw_first_last_order,\n",
    "        ANY_VALUE(users.days_bw_first_last_order) / ANY_VALUE(users.num_orders) AS avg_days_bw_orders,\n",
    "        COUNT(*) / ANY_VALUE(users.num_orders) AS num_products_per_order,\n",
    "        SUM(op.reordered) / SUM(CASE WHEN orders.order_number > 1 THEN 1 ELSE 0 END) AS perc_reorder,\n",
    "        COUNT(DISTINCT op.product_id) AS num_products,\n",
    "        COUNT(DISTINCT products.aisle_id) AS num_aisles,\n",
    "        COUNT(DISTINCT products.department_id) AS num_departments\n",
    "      FROM instacart.orders AS orders\n",
    "      INNER JOIN instacart.order_products__prior AS op USING(order_id)\n",
    "      INNER JOIN instacart.products AS products USING(product_id)\n",
    "      INNER JOIN users USING(user_id)\n",
    "      GROUP BY 1\n",
    "    ), product_features AS (\n",
    "      SELECT product_id, aisle_id, department_id,\n",
    "        num_users / num_users_tot AS perc_users,\n",
    "        num_orders / num_orders_tot AS perc_all_orders,\n",
    "        num_reorder / num_reorder_tot AS perc_reorder\n",
    "      FROM (\n",
    "        SELECT products.product_id, products.aisle_id, products.department_id,\n",
    "          COUNT(DISTINCT orders.user_id) AS num_users,\n",
    "          COUNT(*) AS num_orders, \n",
    "          SUM(op.reordered) AS num_reorder\n",
    "        FROM instacart.orders AS orders\n",
    "        INNER JOIN instacart.order_products__prior AS op USING(order_id)\n",
    "        INNER JOIN instacart.products AS products USING(product_id)\n",
    "        GROUP BY 1,2,3\n",
    "      ) AS x\n",
    "      INNER JOIN (\n",
    "        SELECT COUNT(DISTINCT user_id) AS num_users_tot,\n",
    "          COUNT(*) AS num_orders_tot, \n",
    "          SUM(CASE WHEN order_number > 1 THEN 1 ELSE 0 END) AS num_reorder_tot\n",
    "        FROM instacart.orders\n",
    "        WHERE eval_set = \"prior\"\n",
    "      ) AS y ON 1=1\n",
    "    ), all_features AS (\n",
    "      SELECT\n",
    "        upf.user_id,\n",
    "        upf.product_id,\n",
    "        pf.aisle_id,\n",
    "        pf.department_id,\n",
    "        upf.perc_all_orders AS upf_perc_all_orders,\n",
    "        upf.perc_reorder AS upf_perc_reorder,\n",
    "        upf.orders_per_day AS upf_orders_per_day,\n",
    "        upf.reorders_per_day AS upf_reorders_per_day,\n",
    "        upf.first_order_number AS upf_first_order_number,\n",
    "        upf.first_order_day AS upf_first_order_day,\n",
    "        upf.last_order_number AS upf_last_order_number,\n",
    "        upf.last_order_day AS upf_last_order_day,\n",
    "        upf.avg_cart_order AS upf_avg_cart_order,\n",
    "        uf.num_orders AS uf_num_orders,\n",
    "        uf.num_products_per_order AS uf_num_products_per_order,\n",
    "        uf.perc_reorder AS uf_perc_reorder,\n",
    "        uf.days_bw_first_last_order AS uf_days_bw_first_last_order,\n",
    "        uf.avg_days_bw_orders AS uf_avg_days_bw_orders,\n",
    "        uf.num_products AS uf_num_products,\n",
    "        uf.num_aisles AS uf_num_aisles,\n",
    "        uf.num_departments AS uf_num_departments,\n",
    "        pf.perc_users AS pf_perc_users,\n",
    "        pf.perc_all_orders AS pf_perc_all_orders,\n",
    "        pf.perc_reorder AS pf_perc_reorder\n",
    "      FROM user_product_features AS upf\n",
    "      INNER JOIN user_features AS uf USING(user_id)\n",
    "      INNER JOIN product_features AS pf USING(product_id)\n",
    "    )\n",
    "    SELECT af.*, \n",
    "      # a few other features that need to computed based on order\n",
    "      af.uf_days_bw_first_last_order - af.upf_last_order_day + o.days_since_prior_order AS upf_days_since_last_order,\n",
    "      o.order_number - af.upf_last_order_number AS upf_orders_since_last_order,\n",
    "      # train vs. test and reordered (only for train)\n",
    "      o.eval_set,\n",
    "      o.order_id,\n",
    "      CASE WHEN o.eval_set = \"test\" THEN NULL ELSE LEAST(COALESCE(op_train.order_id,0),1) END AS reordered\n",
    "    FROM all_features AS af\n",
    "    INNER JOIN instacart.orders AS o ON af.user_id = o.user_id AND o.eval_set IN ('train','test')\n",
    "    LEFT JOIN instacart.order_products__train AS op_train ON o.order_id = op_train.order_id AND af.product_id = op_train.product_id\n",
    "\"\"\"\n",
    "\n",
    "query_job = bq_client.query(query, job_config=job_config)\n",
    "query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull data from BQ into Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for deleting temp files when we're done\n",
    "def cleanup(sess, input_directory):\n",
    "    input_path = sess._jvm.org.apache.hadoop.fs.Path(input_directory)\n",
    "    input_path.getFileSystem(sess._jsc.hadoopConfiguration()).delete(input_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up spark session\n",
    "sess = SparkSession.builder.appName(\"Model builder\").getOrCreate()\n",
    "bucket = sess._sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')\n",
    "project = sess._sc._jsc.hadoopConfiguration().get('fs.gs.project.id')\n",
    "input_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_input'.format(bucket)\n",
    "output = 'gs://instacart-data/outputs/reorder_test_pred.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from bq\n",
    "conf = {\n",
    "    'mapred.bq.project.id': project,\n",
    "    'mapred.bq.gcs.bucket': bucket,\n",
    "    'mapred.bq.temp.gcs.path': input_directory,\n",
    "    'mapred.bq.input.project.id': project,\n",
    "    'mapred.bq.input.dataset.id': 'instacart',\n",
    "    'mapred.bq.input.table.id': 'reorder_model',\n",
    "}\n",
    "\n",
    "cleanup(sess, input_directory)\n",
    "\n",
    "data_raw = sess._sc.newAPIHadoopRDD(\n",
    "    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'com.google.gson.JsonObject',\n",
    "    conf=conf)\n",
    "\n",
    "data_json = data_raw.map(lambda x: x[1])\n",
    "data_df = sess.read.json(data_json).repartition(sess._sc.defaultParallelism * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast integers\n",
    "data_df = data_df\\\n",
    "    .withColumn('label', data_df.reordered.cast('integer'))\\\n",
    "    .withColumn('aisle_id', data_df.aisle_id.cast('integer'))\\\n",
    "    .withColumn('department_id', data_df.department_id.cast('integer'))\\\n",
    "    .withColumn('user_id', data_df.user_id.cast('integer'))\\\n",
    "    .withColumn('product_id', data_df.product_id.cast('integer'))\\\n",
    "    .withColumn('order_id', data_df.order_id.cast('integer'))\\\n",
    "    .withColumn('uf_num_orders', data_df.uf_num_orders.cast('integer'))\\\n",
    "    .withColumn('uf_days_bw_first_last_order', data_df.uf_days_bw_first_last_order.cast('integer'))\\\n",
    "    .withColumn('uf_num_aisles', data_df.uf_num_aisles.cast('integer'))\\\n",
    "    .withColumn('uf_num_departments', data_df.uf_num_departments.cast('integer'))\\\n",
    "    .withColumn('uf_num_products', data_df.uf_num_products.cast('integer'))\\\n",
    "    .withColumn('upf_first_order_day', data_df.upf_first_order_day.cast('integer'))\\\n",
    "    .withColumn('upf_first_order_number', data_df.upf_first_order_number.cast('integer'))\\\n",
    "    .withColumn('upf_last_order_day', data_df.upf_last_order_day.cast('integer'))\\\n",
    "    .withColumn('upf_last_order_number', data_df.upf_last_order_number.cast('integer'))\\\n",
    "    .withColumn('upf_orders_since_last_order', data_df.upf_orders_since_last_order.cast('integer'))\\\n",
    "    .withColumn('upf_days_since_last_order', data_df.upf_days_since_last_order.cast('integer'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[aisle_id: int, department_id: int, eval_set: string, order_id: int, pf_perc_all_orders: double, pf_perc_reorder: double, pf_perc_users: double, product_id: int, reordered: string, uf_avg_days_bw_orders: double, uf_days_bw_first_last_order: int, uf_num_aisles: int, uf_num_departments: int, uf_num_orders: int, uf_num_products: int, uf_num_products_per_order: double, uf_perc_reorder: double, upf_avg_cart_order: double, upf_days_since_last_order: int, upf_first_order_day: int, upf_first_order_number: int, upf_last_order_day: int, upf_last_order_number: int, upf_orders_per_day: double, upf_orders_since_last_order: int, upf_perc_all_orders: double, upf_perc_reorder: double, upf_reorders_per_day: double, user_id: int, label: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache to each worker\n",
    "data_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "train = data_df.filter(data_df.eval_set == 'train')\n",
    "test = data_df.filter(data_df.eval_set == 'test')\n",
    "\n",
    "train_user, validate_user = train.select('user_id').distinct().randomSplit([0.8, 0.2], seed=1)\n",
    "train2 = train.join(train_user, 'user_id')\n",
    "validate = train.join(validate_user, 'user_id')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct pipeline\n",
    "xvar1 = [\"upf_perc_all_orders\", \"upf_perc_reorder\", \"upf_orders_per_day\", \"upf_reorders_per_day\", \\\n",
    "         \"upf_first_order_number\", \"upf_first_order_day\", \"upf_last_order_number\", \"upf_last_order_day\", \\\n",
    "         \"upf_avg_cart_order\", \"upf_days_since_last_order\", \"upf_orders_since_last_order\"]\n",
    "\n",
    "xvar2 = [\"uf_num_orders\", \"uf_num_products_per_order\", \"uf_perc_reorder\", \\\n",
    "         \"uf_days_bw_first_last_order\", \"uf_avg_days_bw_orders\", \"uf_num_products\", \"uf_num_aisles\", \\\n",
    "         \"uf_num_departments\"]\n",
    "\n",
    "xvar3 = [\"pf_perc_users\", \"pf_perc_all_orders\", \"pf_perc_reorder\"]\n",
    "\n",
    "xvar = xvar1 + xvar2 + xvar3\n",
    "\n",
    "null_counts = train.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in train.columns)).\\\n",
    "              toPandas().transpose()\n",
    "null_col = list(null_counts.index[null_counts[0].nonzero()])\n",
    "\n",
    "imp = Imputer(strategy=\"median\", inputCols=null_col, outputCols=null_col)\n",
    "va = VectorAssembler(inputCols=xvar, outputCol=\"features\")\n",
    "ss = StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"features2\")\n",
    "lr = LogisticRegression(maxIter=100, featuresCol=\"features2\")\n",
    "pipeline = Pipeline(stages=[imp, va, ss, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.8202132454179041\n",
      "Best CV param: {Param(parent='LogisticRegression_4ec08603d307f531fe6b', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LogisticRegression_4ec08603d307f531fe6b', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "param_grid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, list([10**k for k in range(-2, 0)])) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "eva = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    numFolds=3,\n",
    "                    evaluator=eva)\n",
    "\n",
    "cv_model = cv.fit(train2)\n",
    "\n",
    "best_func = np.argmax if eva.isLargerBetter() else np.argmin\n",
    "best_idx = best_func(cv_model.avgMetrics)\n",
    "best_score = cv_model.avgMetrics[best_idx]\n",
    "best_param = param_grid[best_idx]\n",
    "\n",
    "print(\"Best CV score: {}\".format(best_score))\n",
    "print(\"Best CV param: {}\".format(best_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.19\n",
      "Optimal threshold F1: 0.42373350509153046\n"
     ]
    }
   ],
   "source": [
    "# determine cutoff which maximizes mean F1 score\n",
    "true_prob = udf(lambda x: float(x[-1]))\n",
    "\n",
    "validate_pred = cv_model.transform(validate)\n",
    "validate_pred = validate_pred.select(true_prob('probability').alias('probability').cast('float'), 'label')\n",
    "validate_pred_df = validate_pred.toPandas()\n",
    "\n",
    "thresholds = np.arange(0, 1, 0.01)\n",
    "precision = np.array([np.mean(validate_pred_df.label[validate_pred_df.probability > x]) for x in thresholds])\n",
    "recall = np.array([np.sum(validate_pred_df.label[validate_pred_df.probability > x]) / np.sum(validate_pred_df.label) for x in thresholds])\n",
    "f1 = (2*precision*recall) / (precision + recall)\n",
    "optimal_threshold = thresholds[np.nanargmax(f1)]\n",
    "\n",
    "print(\"Optimal threshold: {}\".format(optimal_threshold))\n",
    "print(\"Optimal threshold F1: {}\".format(np.nanmax(f1)))\n",
    "\n",
    "best_param[lr.threshold] = optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model and prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune model on entire data\n",
    "model = pipeline.fit(train, best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions for test set\n",
    "collapse = udf(lambda x: ' '.join([str(i) for i in x]))\n",
    "\n",
    "test_pred = model.transform(test)\n",
    "test_pred = test_pred.filter(test_pred.prediction == 1)\\\n",
    "                .groupBy('order_id').agg(collect_list('product_id').alias('products'))\n",
    "test_pred = test_pred.withColumn('products', collapse('products'))\n",
    "test_pred = test.select('order_id').distinct().join(test_pred, on='order_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "cleanup(sess, output)\n",
    "test_pred.repartition(1).write.option('header', 'true').csv(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "cleanup(sess, input_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}