{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BQ table with data for model\n",
    "\n",
    "Train and test sets delineated by whether user is in train/test. Validate set contains NEW orders by members in the train set. To simplify model, filter to products comprising top 90% of orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7fcb27a7bcc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_client = bigquery.Client()\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "table_ref = bq_client.dataset('instacart').table('new_prod_model')\n",
    "\n",
    "param = bigquery.ScalarQueryParameter('perc_top_products', 'FLOAT', 0.9)\n",
    "job_config.query_parameters = [param]\n",
    "job_config.destination = table_ref\n",
    "job_config.write_disposition = 'WRITE_TRUNCATE'\n",
    "\n",
    "query = \"\"\"\n",
    "    WITH top_products AS (\n",
    "      SELECT product_id FROM (\n",
    "        SELECT product_id, n_orders,\n",
    "          SUM(n_orders) OVER (ORDER BY n_orders DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS n_orders_cumulative,\n",
    "          (SELECT COUNT(*) FROM instacart.order_products__prior) AS n_orders_total\n",
    "        FROM (\n",
    "          SELECT product_id, COUNT(*) AS n_orders FROM instacart.order_products__prior GROUP BY 1\n",
    "        ) AS x\n",
    "      ) AS x WHERE n_orders_cumulative / n_orders_total <= @perc_top_products\n",
    "    ), prior_products AS (\n",
    "      SELECT o.user_id, opp.product_id\n",
    "      FROM instacart.order_products__prior opp\n",
    "      INNER JOIN instacart.orders o USING(order_id)\n",
    "      GROUP BY 1,2\n",
    "    ), users AS (\n",
    "      SELECT user_id, eval_set, order_id\n",
    "      FROM instacart.orders\n",
    "      WHERE eval_set IN (\"train\", \"test\")\n",
    "      GROUP BY 1,2,3\n",
    "    ), order_products_train AS (\n",
    "      SELECT opp.product_id, o.user_id, u.eval_set, u.order_id\n",
    "      FROM instacart.order_products__prior opp\n",
    "      INNER JOIN top_products tp USING(product_id)\n",
    "      INNER JOIN instacart.orders o USING(order_id)\n",
    "      INNER JOIN users u USING(user_id)\n",
    "    ), order_products_validate AS (\n",
    "      SELECT opt.product_id, o.user_id\n",
    "      FROM instacart.order_products__train opt\n",
    "      INNER JOIN instacart.orders o USING(order_id)\n",
    "      LEFT JOIN prior_products pp USING(user_id, product_id)\n",
    "      WHERE pp.product_id IS NULL\n",
    "    )\n",
    "    SELECT op.user_id, op.product_id, op.eval_set, op.order_id, \n",
    "      COUNT(DISTINCT op.order_id)/ANY_VALUE(u.n_user_orders) AS order_freq\n",
    "    FROM order_products_train AS op\n",
    "    INNER JOIN (\n",
    "      SELECT user_id, COUNT(DISTINCT order_id) AS n_user_orders\n",
    "      FROM order_products_train GROUP BY 1\n",
    "    ) AS u USING(user_id)\n",
    "    GROUP BY 1,2,3,4\n",
    "    UNION ALL\n",
    "    SELECT user_id, product_id, 'validate' AS eval_set, NULL AS order_id, NULL AS order_freq\n",
    "    FROM order_products_validate\n",
    "    GROUP BY 1,2\n",
    "\"\"\"\n",
    "\n",
    "query_job = bq_client.query(query, job_config=job_config)\n",
    "query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull data from BQ into Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for deleting temp files when we're done\n",
    "def cleanup(sess, input_dir):\n",
    "    input_path = sess._jvm.org.apache.hadoop.fs.Path(input_dir)\n",
    "    input_path.getFileSystem(sess._jsc.hadoopConfiguration()).delete(input_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up spark session\n",
    "sess = SparkSession.builder\\\n",
    "    .appName(\"Model builder\")\\\n",
    "    .config(\"spark.executor.cores\", 2)\\\n",
    "    .config(\"spark.executor.memory\", \"7g\")\\\n",
    "    .config(\"spark.network.timeout\", 2000)\\\n",
    "    .config(\"spark.shuffle.io.maxRetries\", 10)\\\n",
    "    .config(\"spark.sql.broadcastTimeout\", 1000)\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "bucket = sess._sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')\n",
    "project = sess._sc._jsc.hadoopConfiguration().get('fs.gs.project.id')\n",
    "input_dir = 'gs://{}/hadoop/tmp/bigquery/pyspark_input'.format(bucket)\n",
    "output = 'gs://instacart-data/outputs/new_prod_test_pred.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from bq\n",
    "conf = {\n",
    "    'mapred.bq.project.id': project,\n",
    "    'mapred.bq.gcs.bucket': bucket,\n",
    "    'mapred.bq.temp.gcs.path': input_dir,\n",
    "    'mapred.bq.input.project.id': project,\n",
    "    'mapred.bq.input.dataset.id': 'instacart',\n",
    "    'mapred.bq.input.table.id': 'new_prod_model',\n",
    "}\n",
    "\n",
    "cleanup(sess, input_dir)\n",
    "\n",
    "data_raw = sess._sc.newAPIHadoopRDD(\n",
    "    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'com.google.gson.JsonObject',\n",
    "    conf=conf)\n",
    "\n",
    "data_json = data_raw.map(lambda x: x[1])\n",
    "data_df = sess.read.json(data_json).repartition(sess._sc.defaultParallelism*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast integers\n",
    "data_df = data_df\\\n",
    "    .withColumn('order_id', data_df.order_id.cast('integer'))\\\n",
    "    .withColumn('user_id', data_df.user_id.cast('integer'))\\\n",
    "    .withColumn('product_id', data_df.product_id.cast('integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Note: must train model once on both train and test members so that we have user-level weights for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "train = data_df.filter(data_df.eval_set.isin(['train','test']))\n",
    "train2 = data_df.filter(data_df.eval_set == 'train')\n",
    "test = data_df.filter(data_df.eval_set == 'test')\n",
    "validate = data_df.filter(data_df.eval_set == 'validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendation model\n",
    "als = ALS()\\\n",
    "    .setMaxIter(10)\\\n",
    "    .setCheckpointInterval(5)\\\n",
    "    .setImplicitPrefs(True)\\\n",
    "    .setSeed(0)\\\n",
    "    .setUserCol(\"user_id\")\\\n",
    "    .setItemCol(\"product_id\")\\\n",
    "    .setRatingCol(\"order_freq\")\\\n",
    "    .setNonnegative(True)\\\n",
    "    .setColdStartStrategy(\"drop\")\\\n",
    "    .setNumBlocks(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.9109078799040278\n",
      "Best CV param: {Param(parent='ALS_47af91b3bc814985358e', name='rank', doc='rank of the factorization'): 100, Param(parent='ALS_47af91b3bc814985358e', name='regParam', doc='regularization parameter (>= 0).'): 0.1}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "train.cache()\n",
    "\n",
    "param_grid = ParamGridBuilder()\\\n",
    "    .addGrid(als.rank, list([10**k for k in range(2, 3)]))\\\n",
    "    .addGrid(als.regParam, list([10**k for k in range(-1, 0)]))\\\n",
    "    .build()\n",
    "\n",
    "eva = RegressionEvaluator(labelCol=\"order_freq\")\n",
    "cv = CrossValidator(estimator=als,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    numFolds=3,\n",
    "                    evaluator=eva)\n",
    "\n",
    "cv_model = cv.fit(train)\n",
    "\n",
    "best_func = np.argmax if eva.isLargerBetter() else np.argmin\n",
    "best_idx = best_func(cv_model.avgMetrics)\n",
    "best_score = cv_model.avgMetrics[best_idx]\n",
    "best_param = param_grid[best_idx]\n",
    "\n",
    "print(\"Best CV score: {}\".format(best_score))\n",
    "print(\"Best CV param: {}\".format(best_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, product_id: int, rating: float]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate predictions for validation set\n",
    "all_pred = cv_model.bestModel.recommendForAllUsers(100)\n",
    "all_pred = all_pred.select(\"user_id\", explode(all_pred.recommendations).alias(\"recommendations\"))\n",
    "all_pred = all_pred.select(\"user_id\", \"recommendations.product_id\", \"recommendations.rating\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove past combinations\n",
    "prior_products = train.select('user_id', 'product_id').distinct().cache()\n",
    "new_pred = all_pred.join(prior_products, on=['user_id', 'product_id'], how=\"left_anti\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag correct predictions\n",
    "validate_pred = new_pred.join(broadcast(train2.select(\"user_id\").distinct()), on=[\"user_id\"])\n",
    "validate_pred = validate_pred.join(broadcast(validate), on=['user_id', 'product_id'], how=\"outer\")\n",
    "validate_pred = validate_pred.withColumn(\"label\", (validate_pred.eval_set == \"validate\").cast(\"integer\"))\n",
    "validate_pred = validate_pred.fillna(-1, subset=[\"rating\"])\n",
    "validate_pred = validate_pred.fillna(0, subset=[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.31\n",
      "Optimal threshold F1: 0.025856346736422006\n"
     ]
    }
   ],
   "source": [
    "# determine cutoff which maximizes mean F1 score\n",
    "validate_pred_agg = validate_pred.withColumn('rating_bkt', round(validate_pred.rating,2))\n",
    "validate_pred_agg = validate_pred_agg.groupBy('rating_bkt')\\\n",
    "                        .agg(sum('label').alias('sum'), count('label').alias('count'))\n",
    "validate_pred_df = validate_pred_agg.toPandas()\n",
    "\n",
    "def precision_fn(df, cutoff): \n",
    "    x = df.loc[df.rating_bkt >= cutoff, ['sum','count']].apply(np.sum)\n",
    "    return x[0] / x[1]\n",
    "\n",
    "def recall_fn(df, cutoff): \n",
    "    return np.sum(df['sum'][df.rating_bkt >= cutoff]) / np.sum(df['sum'])\n",
    "\n",
    "thresholds = np.arange(0, 1, 0.01)\n",
    "precision = np.array([precision_fn(validate_pred_df, x) for x in thresholds])\n",
    "recall = np.array([recall_fn(validate_pred_df, x) for x in thresholds])\n",
    "f1 = (2*precision*recall)/(precision+recall)\n",
    "optimal_threshold = thresholds[np.nanargmax(f1)]\n",
    "\n",
    "print(\"Optimal threshold: {}\".format(optimal_threshold))\n",
    "print(\"Optimal threshold F1: {}\".format(np.nanmax(f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions for test set\n",
    "test_users = test.select(\"user_id\", \"order_id\").distinct().cache()\n",
    "\n",
    "test_pred = new_pred.join(broadcast(test_users), on=[\"user_id\"])\n",
    "test_pred = test_pred.filter(test_pred.rating >= optimal_threshold)\\\n",
    "                .groupBy('order_id').agg(collect_list('product_id').alias('products'))\n",
    "\n",
    "collapse = udf(lambda x: ' '.join([str(i) for i in x]))\n",
    "test_pred = test_pred.withColumn('products', collapse('products'))\n",
    "test_pred = test_users.join(broadcast(test_pred), on='order_id', how='left')\n",
    "test_pred = test_pred.select('order_id', 'products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "cleanup(sess, output)\n",
    "test_pred.repartition(1).write.option('header', 'true').csv(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "cleanup(sess, input_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}